{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a632320",
   "metadata": {},
   "source": [
    "# Feature-Aware Data Augmentation\n",
    "\n",
    "Augmentation policies to consider:\n",
    "- Simple policy randomly sampling of n transforms \n",
    "- Constrained sampling policy with a blacklist of transforms to avoid \n",
    "- Feature-aware augmentation policy where transforms are picked based on their (transform, feature) behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145c3d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "import os\n",
    "import pickle\n",
    "import glob\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from datasets import Dataset, load_dataset, load_from_disk, concatenate_datasets\n",
    "from datasets.utils.logging import disable_progress_bar\n",
    "\n",
    "# amrs\n",
    "import amrlib\n",
    "import penman\n",
    "\n",
    "# transform\n",
    "import sibyl\n",
    "import time\n",
    "import torch\n",
    "import inspect\n",
    "import random\n",
    "from functools import partial\n",
    "\n",
    "# eval pipeline\n",
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "from huggingface_hub import HfApi, ModelFilter\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from scipy.special import softmax\n",
    "\n",
    "# train pipeline\n",
    "import shutil\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification, \n",
    "    AutoTokenizer, \n",
    "    Trainer, \n",
    "    TrainingArguments, \n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "\n",
    "# cleanlab pipeline\n",
    "from cleanlab.filter import find_label_issues\n",
    "from cleanlab.rank import get_label_quality_scores\n",
    "\n",
    "# visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9754eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "evaluate.utils.logging.disable_progress_bar() \n",
    "evaluate.utils.logging.set_verbosity_error()\n",
    "\n",
    "import transformers\n",
    "transformers.utils.logging.disable_progress_bar() \n",
    "transformers.utils.logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184256a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5ae415",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.use_deterministic_algorithms(False)\n",
    "disable_progress_bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fe6e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "\n",
    "def normalize_minmax(df):\n",
    "    return_np = False\n",
    "    if isinstance(df, np.ndarray):\n",
    "        df = pd.DataFrame(df)\n",
    "        return_np = True\n",
    "    for column in df.columns:\n",
    "        df[column] = (df[column] - df[column].min()) / (df[column].max() - df[column].min()) \n",
    "    if return_np:\n",
    "        return df.to_numpy()\n",
    "    return df\n",
    "\n",
    "def normalize_sum(df):\n",
    "    return_np = False\n",
    "    if isinstance(df, np.ndarray):\n",
    "        df = pd.DataFrame(df)\n",
    "        return_np = True\n",
    "    for column in df.columns:\n",
    "        df[column] = df[column] / df[column].sum()\n",
    "    if return_np:\n",
    "        return df.to_numpy()\n",
    "    return df\n",
    "\n",
    "def augment_data(batch, transform, keep_originals=True):\n",
    "    new_texts, new_labels = [], []\n",
    "    for text, label in zip(batch['text'], batch['label']):\n",
    "        new_text, new_label = transform.apply([text], [label])\n",
    "        new_texts.extend(new_text)\n",
    "        new_labels.extend(new_label)\n",
    "    if keep_originals:\n",
    "        return {\"text\": batch['text'] + new_texts, \"label\": batch['label'] + new_labels}\n",
    "    else:\n",
    "        return {\"text\": new_texts, \"label\": new_labels}\n",
    "    \n",
    "def percent_dataset_changed(d1, d2):\n",
    "    return sum([t1['text'] != t2['text'] for t1, t2 in zip(d1, d2)]) / len(d1) \n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    acc = accuracy_score(labels, predictions.argmax(-1))\n",
    "    precision, recall, fbeta_score, support = precision_recall_fscore_support(\n",
    "        y_true=labels, \n",
    "        y_pred=predictions.argmax(-1), \n",
    "        average=\"weighted\", \n",
    "        zero_division=0)\n",
    "    return { 'accuracy': acc , \n",
    "             'precision': precision, \n",
    "             'recall': recall, \n",
    "             'fbeta_score': fbeta_score} \n",
    "\n",
    "def compute_accuracy(predictions, labels):\n",
    "    if not isinstance(labels, np.ndarray):\n",
    "        labels = np.array(labels)\n",
    "    if len(labels.shape) > 1:\n",
    "        acc = acc_at_k(labels, predictions, k=2)       \n",
    "    else:\n",
    "        acc = accuracy_score(labels, np.argmax(predictions, -1))\n",
    "    return acc\n",
    "\n",
    "def vectorize(output):\n",
    "    sorted_output = sorted(output, key=lambda d: d['label']) \n",
    "    probs = torch.tensor([d['score'] for d in sorted_output])\n",
    "    return probs\n",
    "\n",
    "def sample_transforms(transforms, p, n=2, replace=False):\n",
    "    return np.random.choice(transforms, size=n, p=p, replace=replace).tolist()\n",
    "\n",
    "def transforms_to_ids(sampled_transforms, all_transforms):\n",
    "    transforms_ids = [all_transforms.index(i) for i in sampled_transforms]\n",
    "    transforms_applied = np.zeros(len(all_transforms), dtype=np.int32)\n",
    "    transforms_applied[transforms_ids] = 1\n",
    "    return transforms_applied\n",
    "\n",
    "def policy_heatmap(policy, transforms, featurizers):\n",
    "    t_names = [t.transform_class.__name__ for t in transforms]\n",
    "    f_names = [f.__name__ for f in featurizers]\n",
    "    df = pd.DataFrame(policy)\n",
    "    df.columns = f_names\n",
    "    df.index = t_names\n",
    "    sns.heatmap(df)\n",
    "    plt.show()\n",
    "    \n",
    "def implement_policy_probabilities(policy, features):\n",
    "    default_probability = policy.mean(axis=1)\n",
    "    policy_probs = []\n",
    "    for f in features:\n",
    "        available_features = np.nonzero(f)[0]\n",
    "        if len(available_features) == 0:\n",
    "            probs = default_probability\n",
    "        else:\n",
    "            probs = policy[:, available_features].mean(axis=1)\n",
    "        policy_probs.append(probs)\n",
    "    return np.array(policy_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72b9c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transform:\n",
    "    def __init__(self, transform_class, num_outputs=1, task_name=\"sentiment\"):\n",
    "        self.transform_class = transform_class\n",
    "        self.num_outputs = num_outputs\n",
    "        self.task_name = task_name\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.intakes_target = False\n",
    "        self.is_batched = False\n",
    "        \n",
    "        # setting class attributes\n",
    "        if 'to_tense' in inspect.signature(self.transform_class).parameters:\n",
    "            print(f\"initializing {self.transform_class.__name__} with to_tense='past'\") # future & random don't work\n",
    "            self.transform_instance = self.transform_class(to_tense=\"past\")\n",
    "        elif 'source_lang' in inspect.signature(self.transform_class).parameters:\n",
    "            print(f\"initializing {self.transform_class.__name__} with source_lang='es'\") \n",
    "            self.transform_instance = self.transform_class(source_lang=\"es\")\n",
    "        elif 'task_name' in inspect.signature(self.transform_class).parameters:\n",
    "            print(f\"initializing {self.transform_class.__name__} with task_name='{task_name}', return_metadata=True\") \n",
    "            self.transform_instance = self.transform_class(task_name=self.task_name, return_metadata=True)\n",
    "        elif isinstance(self.transform_class, LostInTranslation):\n",
    "            print(f\"initializing {self.transform_class.__name__} with device=0\")\n",
    "            self.transform_instance = self.transform_class(device=0)\n",
    "        else:\n",
    "            print(f\"initializing {self.transform_class.__name__}\")\n",
    "            self.transform_instance = self.transform_class()\n",
    "        \n",
    "        # setting instance attributes\n",
    "        if hasattr(self.transform_instance, \"max_outputs\"):\n",
    "            print(f\"setting max_outputs={self.num_outputs}\")\n",
    "            self.transform_instance.max_outputs = self.num_outputs\n",
    "        if hasattr(self.transform_instance, \"max_paraphrases\"):\n",
    "            print(f\"setting max_paraphrases={self.num_outputs}\")\n",
    "            self.transform_instance.max_paraphrases = self.num_outputs\n",
    "        if hasattr(self.transform_instance, \"device\"):\n",
    "            if self.transform_instance.device is None or self.transform_instance.device == 'cpu':\n",
    "                print(f\"setting device={self.device}\")\n",
    "                self.transform_instance.device = self.device\n",
    "        \n",
    "        # selecting the transformation function\n",
    "        if hasattr(self.transform_class, \"generate\"):\n",
    "            self.transform_fn = self.transform_instance.generate\n",
    "        if hasattr(self.transform_class, \"augment\"):\n",
    "            self.transform_fn = self.transform_instance.augment\n",
    "        if hasattr(self.transform_class, \"transform_batch\"):\n",
    "            self.transform_fn = self.transform_instance.transform_batch\n",
    "            self.intakes_target = True\n",
    "            self.is_batched = True\n",
    "            \n",
    "    def synced_shuffle(self, list1, list2):\n",
    "        # Shuffle two lists with same order\n",
    "        # Using zip() + * operator + shuffle()\n",
    "        temp = list(zip(list1, list2))\n",
    "        random.shuffle(temp)\n",
    "        res1, res2 = zip(*temp)\n",
    "        # res1 and res2 come out as tuples, and so must be converted to lists.\n",
    "        res1, res2 = list(res1), list(res2)\n",
    "        return res1, res2\n",
    "            \n",
    "    def apply(self, texts, labels=None):\n",
    "        if self.intakes_target:\n",
    "            if self.is_batched:\n",
    "                new_texts, new_labels = self.transform_fn((texts, labels))\n",
    "            else:\n",
    "                new_texts, new_labels = [], []\n",
    "                for t, l in zip(texts, labels):\n",
    "                    new_t, new_l = self.transform_fn(t, l)\n",
    "                    new_texts.append(new_t)\n",
    "                    new_labels.extend([new_l] * len(new_t))\n",
    "        else:\n",
    "            if self.is_batched:\n",
    "                new_texts = self.transform_fn((texts))\n",
    "                new_texts = labels\n",
    "            else:\n",
    "                new_texts, new_labels = [], []\n",
    "                for t, l in zip(texts, labels):\n",
    "                    new_t = self.transform_fn(t)\n",
    "                    if len(new_t) > self.num_outputs:\n",
    "                        new_t = new_t[:self.num_outputs]\n",
    "                    new_texts.extend(new_t)\n",
    "                    new_labels.extend([l] * len(new_t))\n",
    "                    \n",
    "        # post processing since some transformations add/remove more new outputs than expected\n",
    "        if len(new_texts) == 0:\n",
    "            print(\"no new_texts, substituting original texts...\")\n",
    "            new_texts = texts\n",
    "        if len(new_labels) == 0:\n",
    "            print(\"no new_labels, substituting original labels...\")\n",
    "            new_labels = labels\n",
    "        new_texts, new_labels = self.synced_shuffle(new_texts, new_labels)\n",
    "        \n",
    "        expected_len = len(texts) * self.num_outputs\n",
    "        new_texts = new_texts[:expected_len]\n",
    "        new_labels = new_labels[:expected_len]\n",
    "        \n",
    "        return new_texts, new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76986b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AMRGraph:\n",
    "    def __init__(self, amr):\n",
    "        self.graph = penman.decode(amr) if not isinstance(amr, penman.graph.Graph) else amr\n",
    "        self.amr_text = penman.encode(self.graph)\n",
    "\n",
    "    def contains_concept(self, concepts):\n",
    "        \"\"\"\n",
    "        Concepts are nodes / instances in the AMR graph.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if not isinstance(concepts, list): concepts = [concepts]\n",
    "            graph_concepts = [t.target for t in self.graph.instances()]\n",
    "            return any(c for c in graph_concepts if c in concepts)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(self.graph, self.amr_text)\n",
    "\n",
    "    def contains_role(self, roles):\n",
    "        \"\"\"\n",
    "        Roles are edges in the AMR graph.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if not isinstance(roles, list): roles = [roles]\n",
    "            graph_roles = [e.role for e in self.graph.edges()]\n",
    "            return any(r for r in graph_roles if r in roles)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(self.graph, self.amr_text)\n",
    "\n",
    "    def contains_attribute(self, attributes):\n",
    "        \"\"\"\n",
    "        Attributes are properties of concept nodes, i.e. relationships to \n",
    "        constant values.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if not isinstance(attributes, list): attributes = [attributes]\n",
    "            graph_attrs = [a.target for a in self.graph.attributes()]\n",
    "            return any(a for a in graph_attrs if a in attributes)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(self.graph, self.amr_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf559e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# attributes =============================================================\n",
    "\n",
    "def contains_imperative(g): return g.contains_attribute(\"imperative\")\n",
    "def contains_exlamation(g): return g.contains_attribute(\"expressive\")\n",
    "def contains_negation(g):   return g.contains_attribute(\"-\")\n",
    "\n",
    "# concepts ===============================================================\n",
    "\n",
    "def contains_conjunctions(g):         return g.contains_concept([\"and\", \"or\", \"contrast-01\", \"either\", \"neither\"])\n",
    "def contains_interrogative_clause(g): return g.contains_concept(\"truth-value\")\n",
    "def contains_question(g):             return g.contains_concept([\"amr-unknown\", \"amr-choice\"])\n",
    "\n",
    "# roles ==================================================================\n",
    "\n",
    "def contains_coreferences(g): return any(r for r in g.amr_text.split() if r in ['i', 'you', 'he', 'she', 'it', 'we', 'they'])\n",
    "def contains_number(g):       return any(a for a in g.graph.attributes() if a.target.isnumeric())\n",
    "\n",
    "def contains_accompanier(g):  return g.contains_role(':accompanier')\n",
    "def contains_age(g):          return g.contains_role(':age')\n",
    "def contains_beneficiary(g):  return g.contains_role(':beneficiary')\n",
    "def contains_concession(g):   return g.contains_role(':concession')\n",
    "def contains_condition(g):    return g.contains_role(':condition')\n",
    "def contains_consist_of(g):   return any(r for r in g.amr_text.split() if r in [':consist-of'])\n",
    "def contains_degree(g):       return g.contains_role(':degree')\n",
    "def contains_destination(g):  return g.contains_role(':destination')\n",
    "def contains_direction(g):    return g.contains_role(':direction')\n",
    "def contains_domain(g):       return g.contains_role(':domain')\n",
    "def contains_duration(g):     return g.contains_role(':duration')\n",
    "def contains_example(g):      return g.contains_role(':example')\n",
    "def contains_extent(g):       return g.contains_role(':extent')\n",
    "def contains_frequency(g):    return g.contains_role(':frequency')\n",
    "def contains_instrument(g):   return g.contains_role(':instrument')\n",
    "# def contains_li(g):           return g.contains_role(':li')\n",
    "def contains_location(g):     return g.contains_role(':location')\n",
    "def contains_manner(g):       return g.contains_role(':manner')\n",
    "def contains_medium(g):       return g.contains_role(':medium')\n",
    "def contains_mod(g):          return g.contains_role(':mod')\n",
    "def contains_mode(g):         return any(a for a in g.graph.attributes() if \":mode\" in a.role)\n",
    "def contains_name(g):         return g.contains_role(':name')\n",
    "def contains_ord(g):          return g.contains_role(':ord')\n",
    "def contains_part(g):         return g.contains_role(':part')\n",
    "def contains_path(g):         return g.contains_role(':path')\n",
    "def contains_polarity(g):     return g.contains_role(':polarity')\n",
    "def contains_polite(g):       return any(r for r in g.amr_text.split() if r in [':polite'])\n",
    "def contains_poss(g):         return g.contains_role(':poss')\n",
    "def contains_purpose(g):      return g.contains_role(':purpose')\n",
    "def contains_quant(g):        return g.contains_role(':quant')\n",
    "def contains_range(g):        return g.contains_role(':range')\n",
    "def contains_scale(g):        return g.contains_role(':scale')\n",
    "def contains_source(g):       return g.contains_role(':source')\n",
    "def contains_subevent(g):     return g.contains_role(':subevent')\n",
    "def contains_time(g):         return g.contains_role(':time')\n",
    "def contains_topic(g):        return g.contains_role(':topic')\n",
    "def contains_unit(g):         return g.contains_role(':unit')\n",
    "# def contains_value(g):        return g.contains_role(':value')\n",
    "def contains_wiki(g):         return g.contains_role(':wiki')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf67b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AMRFeatureExtractor:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.featurizers = featurizers = [    \n",
    "            contains_accompanier,\n",
    "            contains_age,\n",
    "            contains_beneficiary,\n",
    "            contains_concession,\n",
    "            contains_condition,\n",
    "            contains_conjunctions,\n",
    "            contains_consist_of,\n",
    "            contains_coreferences,\n",
    "            contains_degree,\n",
    "            contains_destination,\n",
    "            contains_direction,\n",
    "            contains_domain,\n",
    "            contains_duration,\n",
    "            contains_example,\n",
    "            contains_exlamation,\n",
    "            contains_extent,\n",
    "            contains_frequency,\n",
    "            contains_imperative,\n",
    "            contains_instrument,\n",
    "            contains_interrogative_clause,\n",
    "            contains_location,\n",
    "            contains_manner,\n",
    "            contains_medium,\n",
    "            contains_mod,\n",
    "            contains_mode,\n",
    "            contains_name,\n",
    "            contains_negation,\n",
    "            contains_number,\n",
    "            contains_ord,\n",
    "            contains_part,\n",
    "            contains_path,\n",
    "            contains_polarity,\n",
    "            contains_polite,\n",
    "            contains_poss,\n",
    "            contains_purpose,\n",
    "            contains_quant,\n",
    "            contains_question,\n",
    "            contains_range,\n",
    "            contains_scale,\n",
    "            contains_source,\n",
    "            contains_subevent,\n",
    "            contains_time,\n",
    "            contains_topic,\n",
    "            contains_unit\n",
    "        ]\n",
    "        self.featurizers = sorted(featurizers, key=lambda f: f.__name__)\n",
    "        self.amr_model   = None\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "    def load_amr_model(self, max_sent_len=128):\n",
    "        self.amr_model = amrlib.load_stog_model(max_sent_len=max_sent_len, device=self.device)\n",
    "        \n",
    "    def text_to_amr(self, texts):\n",
    "        if self.amr_model is None:\n",
    "            self.load_amr_model()\n",
    "        amr_penmans = self.amr_model.parse_sents(texts, add_metadata=False, disable_progress=True)\n",
    "        amr_graphs = []\n",
    "        for p in amr_penmans:\n",
    "            try:\n",
    "                amr_graphs.append(AMRGraph(p))\n",
    "            except Exception as e: \n",
    "                print(e)\n",
    "                print(p)\n",
    "                amr_graphs.append(AMRGraph(p))\n",
    "        return amr_graphs\n",
    "    \n",
    "    def generate_feature_matrix(self, graphs):\n",
    "        feature_matrix = []\n",
    "        for g in graphs:\n",
    "            feature_vector = []\n",
    "            for f in self.featurizers:\n",
    "                feature_vector.append(f(g))\n",
    "            feature_matrix.append(feature_vector)\n",
    "        feature_matrix = np.array(feature_matrix, dtype=np.int32)\n",
    "        return feature_matrix\n",
    "    \n",
    "    def __call__(self, texts):\n",
    "        graphs = self.text_to_amr(texts)\n",
    "        return self.generate_feature_matrix(graphs)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11ec405",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Augmenter:\n",
    "    def __init__(self, \n",
    "                 dataset,\n",
    "                 transforms,\n",
    "                 transform_probabilities = None, \n",
    "                 num_augmentations_per_record = 5,\n",
    "                 num_transforms_to_apply = 2,\n",
    "                 batch_size = 10,\n",
    "                 allow_resampling = False,\n",
    "                 keep_originals = False,\n",
    "                 feature_extractor = None,\n",
    "                 perf_extractor = None):\n",
    "        \n",
    "        self.dataset = dataset\n",
    "        self.transforms = transforms\n",
    "        self.transform_probabilities = transform_probabilities\n",
    "        self.num_augmentations_per_record = num_augmentations_per_record\n",
    "        self.num_transforms_to_apply = num_transforms_to_apply\n",
    "        self.batch_size = batch_size\n",
    "        self.allow_resampling = allow_resampling\n",
    "        self.keep_originals = keep_originals\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.perf_extractor = perf_extractor\n",
    "        \n",
    "        # initializations\n",
    "        self.dataset = dataset.remove_columns(\"idx\")\n",
    "        self.add_idx_to_dataset()\n",
    "        self.num_transforms_available = len(self.transforms)\n",
    "        \n",
    "        if self.transform_probabilities is None:\n",
    "            # set to uniform\n",
    "            num_examples = len(self.dataset)\n",
    "            num_transforms = len(self.transforms)\n",
    "            uniform_policy = np.full((num_examples, num_transforms), fill_value=1/num_transforms)\n",
    "            self.transform_probabilities = uniform_policy\n",
    "        \n",
    "    def add_idx_to_dataset(self):\n",
    "        if 'idx' not in self.dataset.features:\n",
    "            self.dataset = self.dataset.add_column(\"idx\", range(len(self.dataset)))\n",
    "        \n",
    "    def apply_to_batch(self, batch):\n",
    "        new_texts, new_labels, transforms_applied, is_changed = [], [], [], []\n",
    "        for idx, text, label in zip(batch['idx'], batch['text'], batch['label']):\n",
    "            actual_batch_size = len(batch['idx'])\n",
    "            original_text, original_label = text, label\n",
    "            for _ in range(self.num_augmentations_per_record):\n",
    "                sampled_transforms = sample_transforms(self.transforms, \n",
    "                                                       p=self.transform_probabilities[idx], \n",
    "                                                       n=self.num_transforms_to_apply, \n",
    "                                                       replace=self.allow_resampling)\n",
    "                transforms_applied.append(transforms_to_ids(sampled_transforms, self.transforms))\n",
    "                for t in sampled_transforms:\n",
    "                    try:\n",
    "                        text, label = t.apply([text], [label])\n",
    "                        text, label = text[0], label[0]\n",
    "                    except Exception as e: \n",
    "                        print(e)\n",
    "                        print(f\"[Augmenter]: skipping augmentation from {t.transform_class.__name__} on text:'{text}' and label: {label}\")\n",
    "\n",
    "                # avoid adding records with empty text\n",
    "                if text:\n",
    "                    new_texts.append(text)\n",
    "                    new_labels.append(label)\n",
    "                    is_changed.append(int(original_text != text))\n",
    "\n",
    "        if self.keep_originals:\n",
    "            new_texts = batch['text'] + new_texts\n",
    "            new_labels = batch['label'] + new_labels\n",
    "            realized_batch_size = len(new_labels)\n",
    "            transforms_applied = transforms_applied + np.zeros((actual_batch_size, len(self.transforms)), dtype=np.int32).tolist()\n",
    "            is_changed = is_changed + [0] * actual_batch_size\n",
    "            out = {\n",
    "                \"text\": new_texts, \n",
    "                \"label\": new_labels,\n",
    "                \"idx\": list(range(realized_batch_size)),\n",
    "                \"transforms_applied\": [t for t in transforms_applied],\n",
    "                \"is_changed\": is_changed\n",
    "            }\n",
    "        else:\n",
    "            out = {\n",
    "                \"text\": new_texts, \n",
    "                \"label\": new_labels, \n",
    "                \"idx\": list(range(len(new_labels))),\n",
    "                \"transforms_applied\": transforms_applied,\n",
    "                \"is_changed\": is_changed\n",
    "            }\n",
    "\n",
    "        return out\n",
    "            \n",
    "                                                   \n",
    "    def augment(self):\n",
    "        dataset = self.dataset.map(self.apply_to_batch, batched=True, batch_size=self.batch_size)\n",
    "        dataset = dataset.remove_columns(\"idx\")\n",
    "    \n",
    "        # feature extraction\n",
    "        if self.feature_extractor is not None:\n",
    "            features = self.feature_extractor(dataset[\"text\"])\n",
    "            dataset = dataset.add_column(\"features\", [f for f in features])\n",
    "                \n",
    "        # performance scoring    \n",
    "        if self.perf_extractor is not None:\n",
    "            performances = self.perf_extractor(dataset[\"text\"], dataset[\"label\"])\n",
    "            dataset = dataset.add_column(\"performance\", [p for p in performances])\n",
    "                \n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed718c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Likelihood:\n",
    "    def __init__(self):\n",
    "        self.scorer = torch.nn.NLLLoss(reduction=\"none\")\n",
    "    \n",
    "    def __call__(self, probs, targets, indices=None):\n",
    "        return -self.scorer(probs, targets).numpy()\n",
    "    \n",
    "class InverseLikelihood:\n",
    "    def __init__(self):\n",
    "        self.scorer = torch.nn.NLLLoss(reduction=\"none\")\n",
    "    \n",
    "    def __call__(self, probs, targets, indices=None):\n",
    "        return 1+self.scorer(probs, targets).numpy()\n",
    "    \n",
    "class CleanLabSafe:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def __call__(self, probs, targets, indices=None):\n",
    "        probs = probs.numpy()\n",
    "        targets = targets.numpy()\n",
    "        scores = ~find_label_issues(\n",
    "            labels=targets,\n",
    "            pred_probs=probs,\n",
    "            n_jobs=1\n",
    "        )\n",
    "        return scores.astype(np.int32).tolist()\n",
    "    \n",
    "class CleanLabQualityScore:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def __call__(self, probs, targets, indices=None):\n",
    "        probs = probs.numpy()\n",
    "        targets = targets.numpy()\n",
    "        scores = get_label_quality_scores(\n",
    "            labels=targets,\n",
    "            pred_probs=probs,\n",
    "            n_jobs=1\n",
    "        )\n",
    "        return scores.astype(np.int32).tolist()\n",
    "    \n",
    "class LikelihoodShift:\n",
    "    def __init__(self, original_dataset, direction=\"positive\"):\n",
    "        self.original_dataset = original_dataset\n",
    "        self.direction = direction\n",
    "        self.scorer = torch.nn.NLLLoss(reduction=\"none\")\n",
    "        \n",
    "    def __call__(self, probs, targets, indices=None):\n",
    "        new_scores  = -self.scorer(probs, targets).numpy()\n",
    "        \n",
    "        old_probs   = torch.tensor(self.original_dataset.select(indices)[\"preds\"])\n",
    "        old_targets = torch.tensor(self.original_dataset.select(indices)['label'])\n",
    "        old_scores  = -self.scorer(old_probs, old_targets).numpy()\n",
    "            \n",
    "        if self.direction in \"positive\":\n",
    "            scores = (new_scores - old_scores).clip(0, 1)\n",
    "        elif self.direction in \"negative\":\n",
    "            scores = (old_scores - new_scores).clip(0, 1)\n",
    "        else:\n",
    "            scores = new_scores\n",
    "        return scores\n",
    "        \n",
    "class PerformanceExtractor:\n",
    "    def __init__(self, dataset_name, scorer, model_id=None):\n",
    "        self.dataset_name = dataset_name\n",
    "        self.scorer = scorer\n",
    "        self.model_id = model_id\n",
    "        self.api = HfApi()\n",
    "        self.pipe = None\n",
    "        self.device = 0 if torch.cuda.is_available() else -1\n",
    "\n",
    "        if self.model_id and not self.pipe:\n",
    "            self.create_pipe(self.model_id)\n",
    "\n",
    "        if not self.pipe:\n",
    "            self.find_model_for_dataset()\n",
    "\n",
    "    def create_pipe(self, model_id):\n",
    "        self.pipe = pipeline(\"text-classification\", \n",
    "                            model=model_id, \n",
    "                            device=self.device, \n",
    "                            padding=True, \n",
    "                            truncation=True,\n",
    "                            top_k=None)\n",
    "        return self.pipe\n",
    "\n",
    "    def find_model_for_dataset(self):\n",
    "        model_filter = ModelFilter(\n",
    "            task=\"text-classification\",\n",
    "            library=\"pytorch\",\n",
    "            # model_name=dataset_name,\n",
    "            trained_dataset=self.dataset_name)\n",
    "        model_id = next(iter(self.api.list_models(filter=model_filter)))\n",
    "        if model_id:\n",
    "            model_id = getattr(model_id, 'modelId')\n",
    "            print('Using ' + model_id + ' to support evaluation.')\n",
    "            self.create_pipe(model_id)\n",
    "\n",
    "    def extract_prediction_probabilities(self, inputs):\n",
    "        output = self.pipe(inputs)\n",
    "        return torch.stack([vectorize(o) for o in output])\n",
    "    \n",
    "    def extract_prediction_classes(self, inputs):\n",
    "        return torch.argmax(self.extract_prediction_probabilities(inputs), axis=1)\n",
    "\n",
    "    def __call__(self, inputs, targets, indices=None):\n",
    "        probs   = self.extract_prediction_probabilities(inputs)\n",
    "        targets = torch.tensor(targets)\n",
    "        return self.scorer(probs, targets, indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cabf8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition_dataset_by_class(dataset):\n",
    "    classes = dataset.features['label'].names\n",
    "    num_classes = len(classes)\n",
    "\n",
    "    class_partitions = []\n",
    "    for i in range(num_classes):\n",
    "        class_partition = dataset.filter(lambda row: row[\"label\"] == i)\n",
    "        class_partitions.append(class_partition)\n",
    "    return class_partitions\n",
    "\n",
    "def balance_dataset(dataset, num_per_class=100):\n",
    "    # partition dataset by class\n",
    "    class_partitions = partition_dataset_by_class(dataset)\n",
    "\n",
    "    # find smallest number of instances among any class\n",
    "    if \"min\" in str(num_per_class):\n",
    "        smallest_num_instances = min([len(p) for p in class_partitions])\n",
    "        print(f\"original num_per_class: {num_per_class}, new num_per_class: {smallest_num_instances}\")\n",
    "        num_per_class = smallest_num_instances\n",
    "\n",
    "    # filter to desired amount\n",
    "    filtered_partitions = []\n",
    "    for class_partition in class_partitions:\n",
    "        # select only the requested amount\n",
    "        num_instances_in_class = len(class_partition)\n",
    "        if num_instances_in_class >= num_per_class:\n",
    "            idx_to_keep = random.sample(range(num_instances_in_class), num_per_class)\n",
    "            class_partition = class_partition.select(idx_to_keep).shuffle()\n",
    "        filtered_partitions.append(class_partition)\n",
    "    return concatenate_datasets(filtered_partitions)\n",
    "\n",
    "def deduplicate(dataset):\n",
    "    df = pd.DataFrame(dataset)\n",
    "    df = df.drop_duplicates(subset=[\"text\", \"label\"])\n",
    "    return Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e5d8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlignmentMetric:\n",
    "    \"\"\"\n",
    "    Use cleanlab to generate a label alignment score.  \n",
    "    :Package Requirements:\n",
    "        * pip install cleanlab\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.api = HfApi()\n",
    "        self.device = 0 if torch.cuda.is_available() else -1\n",
    "        self.pipe = None\n",
    "        self.save_name = \"alignment_score\"\n",
    "\n",
    "    def find_model_for_dataset(self, dataset_name):\n",
    "        \n",
    "        model_filter = ModelFilter(\n",
    "            task=\"text-classification\",\n",
    "            library=\"pytorch\",\n",
    "            # model_name=dataset_name,\n",
    "            trained_dataset=dataset_name)\n",
    "\n",
    "        model_id = next(iter(self.api.list_models(filter=model_filter)))\n",
    "\n",
    "        if model_id:\n",
    "            model_id = getattr(model_id, 'modelId')\n",
    "            print('Using ' + model_id + ' to support cleanlab datalabel issues.')\n",
    "            self.pipe = pipeline(\"text-classification\", \n",
    "                                 model=model_id, \n",
    "                                 device=self.device, \n",
    "                                 top_k=None)\n",
    "\n",
    "    def extract_prediction_probabilities(self, dataset):\n",
    "        if self.pipe is None:\n",
    "            self.find_model_for_dataset(dataset.config_name)\n",
    "        output = self.pipe(dataset['text'])\n",
    "        return np.stack([vectorize(o) for o in output])\n",
    "\n",
    "    def evaluate(self, dataset, annotate_dataset=False):\n",
    "        pred_probs = self.extract_prediction_probabilities(dataset)\n",
    "        scores = get_label_quality_scores(\n",
    "            labels=dataset['label'],\n",
    "            pred_probs=pred_probs,  \n",
    "        )\n",
    "        if annotate_dataset:\n",
    "            if self.save_name in dataset.features:\n",
    "                dataset = dataset.remove_columns([self.save_name])\n",
    "            dataset = dataset.add_column(self.save_name, [s for s in scores])\n",
    "        return dataset, np.array(scores)\n",
    "    \n",
    "    def evaluate_before_and_after(self, before_dataset, after_dataset, annotate_after_dataset=True):\n",
    "        \"\"\"\n",
    "        Higher is better. Anything lower than 1 means that the\n",
    "        changes made to the text reduced label alignment. \n",
    "        \"\"\"\n",
    "        before_dataset, before_scores = self.evaluate(before_dataset)\n",
    "        after_dataset, after_scores   = self.evaluate(after_dataset)\n",
    "        scores = np.nan_to_num(after_scores / before_scores)\n",
    "        if annotate_after_dataset:\n",
    "            if self.save_name in after_dataset.features:\n",
    "                after_dataset = after_dataset.remove_columns([self.save_name])\n",
    "            after_dataset = after_dataset.add_column(self.save_name, [s for s in scores])\n",
    "        return after_dataset, scores\n",
    "            \n",
    "\n",
    "class FluencyMetric:\n",
    "    def __init__(self, model_id='gpt2') -> None:\n",
    "        \"\"\"\n",
    "        Use gpt2 to measure how perplexing / surprising a given text is \n",
    "        to a well trained language model. When used on text that we know\n",
    "        is natural / human sounding, then perplexity is a measure of \n",
    "        model quality. However, when we trust that the language model is\n",
    "        pretty good already and we aren't sure about the quality of the \n",
    "        text, then we can use perplexity to measure text naturalness. \n",
    "        :Package Requirements:\n",
    "            * pip install evaluate\n",
    "        :Language: english\n",
    "        \"\"\"\n",
    "        import evaluate\n",
    "        self.model_id = model_id\n",
    "        self.metric = evaluate.load(\"perplexity\", module_type=\"metric\")\n",
    "        self.save_name = \"fluency_score\"\n",
    "    \n",
    "    def evaluate(self, dataset, annotate_dataset=False):\n",
    "        scores = self.metric.compute(\n",
    "            predictions=dataset['text'], \n",
    "            model_id=self.model_id)['perplexities']\n",
    "        if annotate_dataset:\n",
    "            if self.save_name in dataset.features:\n",
    "                dataset = dataset.remove_columns([self.save_name])\n",
    "            dataset = dataset.add_column(self.save_name, [s for s in scores])\n",
    "        return dataset, np.array(scores)\n",
    "    \n",
    "    def evaluate_before_and_after(self, before_dataset, after_dataset, annotate_after_dataset=True):\n",
    "        \"\"\"\n",
    "        Higher is better. Anything lower than 1 means that the\n",
    "        changes made to the text reduced fluency. \n",
    "        \"\"\"\n",
    "        before_dataset, before_scores = self.evaluate(before_dataset)\n",
    "        after_dataset, after_scores   = self.evaluate(after_dataset)\n",
    "        scores = np.nan_to_num(before_scores / after_scores)\n",
    "        if annotate_after_dataset:\n",
    "            if self.save_name in after_dataset.features:\n",
    "                after_dataset = after_dataset.remove_columns([self.save_name])\n",
    "            after_dataset = after_dataset.add_column(self.save_name, [s for s in scores])\n",
    "        return after_dataset, scores\n",
    "    \n",
    "class GrammarMetric:\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"\n",
    "        Use language_tool_python to check grammer.\n",
    "        :Package Requirements:\n",
    "            * pip install language_tool_python\n",
    "        :Language: english\n",
    "        \"\"\"\n",
    "        import language_tool_python\n",
    "        self.language_tool = language_tool_python.LanguageTool('en-US')\n",
    "        self.save_name = \"grammar_score\"\n",
    "\n",
    "    def find_grammar_issues(self, text):\n",
    "        return self.language_tool.check(text)\n",
    "\n",
    "    def correct_grammar_issues(self, text):\n",
    "        return self.language_tool.correct(text)\n",
    "    \n",
    "    def evaluate(self, dataset, annotate_dataset=False):\n",
    "        scores = [len(self.find_grammar_issues(t)) for t in dataset['text']]\n",
    "        if annotate_dataset:\n",
    "            if self.save_name in dataset.features:\n",
    "                dataset = dataset.remove_columns([self.save_name])\n",
    "            dataset = dataset.add_column(self.save_name, [s for s in scores])\n",
    "        return dataset, np.array(scores)\n",
    "    \n",
    "    def evaluate_before_and_after(self, before_dataset, after_dataset, annotate_after_dataset=True):\n",
    "        \"\"\"\n",
    "        Higher is better. Anything lower than 1 means that the\n",
    "        changes made to the text reduced grammaticality. \n",
    "        \"\"\"\n",
    "        before_dataset, before_scores = self.evaluate(before_dataset)\n",
    "        after_dataset, after_scores   = self.evaluate(after_dataset)\n",
    "        scores = np.nan_to_num(before_scores / after_scores)\n",
    "        if annotate_after_dataset:\n",
    "            if self.save_name in after_dataset.features:\n",
    "                after_dataset = after_dataset.remove_columns([self.save_name])\n",
    "            after_dataset = after_dataset.add_column(self.save_name, [s for s in scores])\n",
    "        return after_dataset, scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f219e112",
   "metadata": {},
   "source": [
    "# TFIM / Cleanlab Suspiciousness Ranking\n",
    "\n",
    "1. Augment some data using a uniform sampling policy. Annotate features / transforms applied.\n",
    "2. Create a TFIM for ranking specifically.\n",
    "    - Label Alignment (i.e. Correctness)\n",
    "    - Grammaticality\n",
    "    - Fluency\n",
    "3. Use features + transforms applied to index into the TFIM.\n",
    "4. Aggrgated queried TFIM values into a suspiciousness score. \n",
    "5. Annotate dataset with suspiciousness scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95507eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from huggingface_hub import HfApi, ModelFilter\n",
    "from cleanlab.rank import get_label_quality_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e99b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_config = (\"glue\", \"sst2\")\n",
    "task_name = \"sentiment\"\n",
    "\n",
    "dataset = load_dataset(*dataset_config, split=\"train\")\n",
    "dataset = dataset.rename_column(\"sentence\", \"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a9658f",
   "metadata": {},
   "outputs": [],
   "source": [
    "blacklist = [\n",
    "    sibyl.Emojify,\n",
    "    sibyl.AddPositiveEmoji,\n",
    "    sibyl.AddNegativeEmoji,\n",
    "    sibyl.Demojify,\n",
    "    sibyl.RemovePositiveEmoji,\n",
    "    sibyl.RemoveNegativeEmoji,\n",
    "    sibyl.AddPositiveEmoji,\n",
    "    sibyl.AddNegativeEmoji,\n",
    "    sibyl.InsertPositivePhrase,\n",
    "    sibyl.InsertNegativePhrase,\n",
    "    sibyl.AddPositiveLink,\n",
    "    sibyl.AddNegativeLink,\n",
    "    sibyl.ImportLinkText,\n",
    "    sibyl.AddNegation,\n",
    "    sibyl.RemoveNegation,\n",
    "    sibyl.ChangeAntonym,\n",
    "    sibyl.ConceptMix,\n",
    "    sibyl.TextMix,\n",
    "    sibyl.SentMix,\n",
    "    sibyl.WordMix,\n",
    "    sibyl.Concept2Sentence\n",
    "]\n",
    "transforms = [t for t in sibyl.TRANSFORMATIONS if t not in blacklist]\n",
    "transforms = sorted(transforms, key=lambda t: t.__name__)\n",
    "transforms = [Transform(t, task_name=task_name) for t in transforms]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9284e8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = AMRFeatureExtractor()\n",
    "a_metric = AlignmentMetric()\n",
    "f_metric = FluencyMetric()\n",
    "g_metric = GrammarMetric()\n",
    "\n",
    "c_a, c_f, c_g = 0.8, 0.1, 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7e6af7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running fada-v3.3 policy search\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset glue (C:/Users/sleev/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b88eddd5da23453db3f510f9d595a904",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/880 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n",
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n",
      "Using pad_token, but it is not set yet.\n"
     ]
    }
   ],
   "source": [
    "dataset_config = (\"glue\", \"sst2\")\n",
    "task_name = \"sentiment\"\n",
    "\n",
    "print(f\"running fada-v3.3 policy search\")\n",
    "\n",
    "dataset = load_dataset(*dataset_config, split=\"train\")\n",
    "dataset = dataset.rename_column(\"sentence\", \"text\")\n",
    "\n",
    "# initialize dataset + annotations\n",
    "if os.path.exists(\"./eval/datasets/glue.sst2.annotated\"):\n",
    "    dataset = load_from_disk(\"./eval/datasets/glue.sst2.annotated\")\n",
    "    features = np.array(dataset[\"features\"])\n",
    "else:\n",
    "    features = feature_extractor(dataset[\"text\"])\n",
    "    preds = perf_extractor.extract_prediction_probabilities(dataset[\"text\"])\n",
    "    dataset = dataset.add_column(\"features\", [f for f in features])\n",
    "    dataset = dataset.add_column(\"preds\", [p.numpy() for p in preds])\n",
    "    dataset.save_to_disk(\"./eval/datasets/glue.sst2.annotated\")\n",
    "\n",
    "# initialize save directory\n",
    "save_dir       = f\"./eval/fadata/sibyl/v3.3/\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# initialize fadata arrays\n",
    "num_rows       = len(dataset)\n",
    "num_transforms = len(transforms)\n",
    "num_features   = len(feature_extractor.featurizers)\n",
    "\n",
    "alignment_scores = np.zeros((num_transforms, num_features))\n",
    "fluency_scores   = np.zeros((num_transforms, num_features))\n",
    "grammar_scores   = np.zeros((num_transforms, num_features))\n",
    "counts           = np.zeros((num_transforms, num_features))\n",
    "changes          = np.zeros((num_transforms, num_features))\n",
    "tfim             = np.full((num_transforms, num_features), fill_value=1/num_transforms)\n",
    "\n",
    "min_coverage = 128\n",
    "num_to_transform_per_step = 128\n",
    "\n",
    "tfim_difference = np.inf\n",
    "convergence_threshold = 1 / (num_transforms + num_features)\n",
    "\n",
    "# run fada-v3.3 convergence loop\n",
    "\n",
    "i = 0\n",
    "while tfim_difference > convergence_threshold:\n",
    "\n",
    "    # find low coverage (t,f) pairs\n",
    "    ts, fs   = np.where(changes < min_coverage)\n",
    "    tf_pairs = list(zip(ts, fs))\n",
    "\n",
    "    for t, f in tqdm(tf_pairs):\n",
    "\n",
    "        f_candidates  = np.where(features[:,f] == 1)[0]\n",
    "\n",
    "        # feature missing in dataset\n",
    "        if not f_candidates.size:\n",
    "            continue\n",
    "\n",
    "        num_to_sample = num_to_transform_per_step if len(f_candidates) > num_to_transform_per_step else len(f_candidates)\n",
    "        f_indices     = np.random.choice(f_candidates, num_to_sample, replace=False)\n",
    "        f_dataset     = dataset.select(f_indices)\n",
    "\n",
    "        t_prob = np.zeros(num_transforms)\n",
    "        t_prob[t] = 1\n",
    "        transform_probabilities = np.array([t_prob for _ in range(f_dataset.num_rows)])\n",
    "        print(\"\")\n",
    "        augmenter = Augmenter(dataset=f_dataset, \n",
    "                      transforms=transforms,  \n",
    "                      transform_probabilities=transform_probabilities,\n",
    "                      num_augmentations_per_record=1,\n",
    "                      num_transforms_to_apply=1,\n",
    "                      batch_size=10, \n",
    "                      keep_originals=False)\n",
    "        aug_dataset = augmenter.augment()\n",
    "\n",
    "        aug_dataset, a_scores = a_metric.evaluate_before_and_after(f_dataset, aug_dataset)\n",
    "        aug_dataset, f_scores = f_metric.evaluate_before_and_after(f_dataset, aug_dataset)\n",
    "        aug_dataset, g_scores = g_metric.evaluate_before_and_after(f_dataset, aug_dataset)\n",
    "\n",
    "        alignment_scores[t,f] = np.clip((alignment_scores[t,f] + a_scores.mean()) / 2, 0, 2)\n",
    "        fluency_scores[t,f]   = np.clip((fluency_scores[t,f]   + f_scores.mean()) / 2, 0, 2)\n",
    "        grammar_scores[t,f]   = np.clip((grammar_scores[t,f]   + g_scores.mean()) / 2, 0, 2)\n",
    "        counts[t,f]           += f_dataset.num_rows\n",
    "        changes[t,f]          += np.array(aug_dataset[\"is_changed\"]).sum()\n",
    "\n",
    "    # compute tfim-augment\n",
    "    aggregated_performance = (c_a * alignment_scores) + (c_f * fluency_scores) + (c_g * grammar_scores)\n",
    "    applicability_rate     = np.nan_to_num(changes / counts, 0)\n",
    "    new_tfim               = softmax(applicability_rate * aggregated_performance, axis=0)\n",
    "\n",
    "    tfim_difference        = np.linalg.norm(new_tfim - tfim)\n",
    "    tfim                   = new_tfim\n",
    "\n",
    "    print(f\"tfim_difference: {tfim_difference} (convergence_threshold: {convergence_threshold})\")\n",
    "\n",
    "    policy_heatmap(tfim, transforms, feature_extractor.featurizers)\n",
    "\n",
    "    print(\"Saving intermediate matrices...\")\n",
    "    np.save(os.path.join(save_dir, f\"glue.sst2.fada.v3.3.counts-step-{i}\"), counts)\n",
    "    np.save(os.path.join(save_dir, f\"glue.sst2.fada.v3.3.changes-step-{i}\"), changes)\n",
    "    np.save(os.path.join(save_dir, f\"glue.sst2.fada.v3.3.alignment-step-{i}\"), alignment_scores)\n",
    "    np.save(os.path.join(save_dir, f\"glue.sst2.fada.v3.3.fluency-step-{i}\"), fluency_scores)\n",
    "    np.save(os.path.join(save_dir, f\"glue.sst2.fada.v3.3.grammar-step-{i}\"), grammar_scores)\n",
    "    np.save(os.path.join(save_dir, f\"glue.sst2.fada.v3.3.tfim-step-{i}\"), tfim)\n",
    "\n",
    "    i += 1\n",
    "    \n",
    "    if i > 12:\n",
    "        break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea5851f",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "a = np.load(os.path.join(save_dir, f\"glue.sst2.fada.v3.2.alignment-step-{i}.npy\"))\n",
    "f = np.load(os.path.join(save_dir, f\"glue.sst2.fada.v3.2.fluency-step-{i}.npy\"))\n",
    "g = np.load(os.path.join(save_dir, f\"glue.sst2.fada.v3.2.grammar-step-{i}.npy\"))\n",
    "c = np.load(os.path.join(save_dir, f\"glue.sst2.fada.v3.2.counts-step-{i}.npy\"))\n",
    "t = np.load(os.path.join(save_dir, f\"glue.sst2.fada.v3.2.tfim-step-{i}.npy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e9eba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "a2 = np.load(os.path.join(save_dir, f\"glue.sst2.fada.v3.2.alignment-step-{i}.npy\"))\n",
    "f2 = np.load(os.path.join(save_dir, f\"glue.sst2.fada.v3.2.fluency-step-{i}.npy\"))\n",
    "g2 = np.load(os.path.join(save_dir, f\"glue.sst2.fada.v3.2.grammar-step-{i}.npy\"))\n",
    "c2 = np.load(os.path.join(save_dir, f\"glue.sst2.fada.v3.2.counts-step-{i}.npy\"))\n",
    "t2 = np.load(os.path.join(save_dir, f\"glue.sst2.fada.v3.2.tfim-step-{i}.npy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01a82b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "c_a, c_f, c_g = 0.75, 0.1, 0.15\n",
    "\n",
    "for i in range(10):\n",
    "    \n",
    "    print(i)\n",
    "    \n",
    "    a = np.load(os.path.join(save_dir, f\"glue.sst2.fada.v3.2.alignment-step-{i}.npy\"))\n",
    "    f = np.load(os.path.join(save_dir, f\"glue.sst2.fada.v3.2.fluency-step-{i}.npy\"))\n",
    "    g = np.load(os.path.join(save_dir, f\"glue.sst2.fada.v3.2.grammar-step-{i}.npy\"))\n",
    "    c = np.load(os.path.join(save_dir, f\"glue.sst2.fada.v3.2.counts-step-{i}.npy\"))\n",
    "    t = np.load(os.path.join(save_dir, f\"glue.sst2.fada.v3.2.tfim-step-{i}.npy\"))\n",
    "    \n",
    "    sns.heatmap(a)\n",
    "    plt.show()\n",
    "    sns.heatmap(a)\n",
    "    plt.show()\n",
    "    sns.heatmap(g)\n",
    "    plt.show()\n",
    "    sns.heatmap(c)\n",
    "    plt.show()\n",
    "    sns.heatmap(t)\n",
    "    plt.show()\n",
    "    sns.heatmap((c_a * a) + (c_f * f) + (c_g * g))\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d263439d",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_a, c_f, c_g = 0.8, 0.1, 0.1\n",
    "\n",
    "TFIM = (c_a * a) + (c_f * f) + (c_g * g)\n",
    "\n",
    "sns.heatmap(TFIM)\n",
    "\n",
    "np.save(\"./eval/TFIM-ranking.npy\", TFIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b59abf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Augment dataset\n",
    "feature_extractor = AMRFeatureExtractor()\n",
    "\n",
    "dataset = load_dataset(*dataset_config, split=\"train\").select(range(100))\n",
    "dataset = dataset.rename_column(\"sentence\", \"text\")\n",
    "\n",
    "augmenter = Augmenter(\n",
    "    dataset=dataset, \n",
    "    transforms=transforms, \n",
    "    transform_probabilities=None, \n",
    "    num_augmentations_per_record=1,\n",
    "    num_transforms_to_apply = 1,\n",
    "    keep_originals=False,\n",
    "    feature_extractor=feature_extractor)\n",
    "uni_dataset = augmenter.augment()\n",
    "\n",
    "transforms_applied = np.array(uni_dataset[\"transforms_applied\"], dtype=np.bool)\n",
    "features = np.array(uni_dataset[\"features\"], dtype=np.bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e0a859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Load a TFIM\n",
    "# tfim = np.load(sorted(glob.glob(f\"./eval/fadata/sibyl/v2-*/*policy*\"))[-1])\n",
    "TFIM = np.load(\"./eval/TFIM-ranking.npy\")\n",
    "policy_heatmap(TFIM, transforms, feature_extractor.featurizers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad64a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3-5. Query TFIM, aggregagate score, save to dataset\n",
    "class TFIMQualityScorer:\n",
    "    def __init__(self, tfim):\n",
    "        self.tfim = tfim\n",
    "        \n",
    "    def score_record(self, transforms_applied, features):\n",
    "        # type check\n",
    "        if not isinstance(transforms_applied, np.ndarray):\n",
    "            transforms_applied = np.array(transforms_applied, dtype=np.bool)\n",
    "        if not isinstance(features, np.ndarray):\n",
    "            features = np.array(features, dtype=np.bool)\n",
    "        # empty check\n",
    "        # if no transforms applied / features present, then average over all values\n",
    "        if np.count_nonzero(features) == 0:\n",
    "            features = np.full_like(features, fill_value=True)\n",
    "        if np.count_nonzero(transforms_applied) == 0:\n",
    "            transforms_applied = np.full_like(transforms_applied, fill_value=True)\n",
    "        # query tfim\n",
    "        return self.tfim[transforms_applied, features].mean()\n",
    "    \n",
    "    def score_dataset(self, dataset):\n",
    "        scores = [self.score_record(r[\"transforms_applied\"], r[\"features\"]) for r in dataset]\n",
    "        if \"tfim_quality_score\" in dataset.features:\n",
    "            dataset= dataset.remove_columns([\"tfim_quality_score\"])\n",
    "        dataset = dataset.add_column(\"tfim_quality_score\", [s for s in scores])\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3761f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(output):\n",
    "    sorted_output = sorted(output, key=lambda d: d['label']) \n",
    "    probs = np.array([d['score'] for d in sorted_output])\n",
    "    return probs\n",
    "\n",
    "\n",
    "class CleanLabQualityScorer:\n",
    "    def __init__(self):\n",
    "        self.api = HfApi()\n",
    "        self.device = 0 if torch.cuda.is_available() else -1\n",
    "        self.pipe = None\n",
    "\n",
    "    def find_model_for_dataset(self, dataset_name):\n",
    "        \n",
    "        model_filter = ModelFilter(\n",
    "            task=\"text-classification\",\n",
    "            library=\"pytorch\",\n",
    "            # model_name=dataset_name,\n",
    "            trained_dataset=dataset_name)\n",
    "\n",
    "        model_id = next(iter(self.api.list_models(filter=model_filter)))\n",
    "\n",
    "        if model_id:\n",
    "            model_id = getattr(model_id, 'modelId')\n",
    "            print('Using ' + model_id + ' to support cleanlab datalabel issues.')\n",
    "            self.pipe = pipeline(\"text-classification\", \n",
    "                                 model=model_id, \n",
    "                                 device=self.device, \n",
    "                                 top_k=None)\n",
    "\n",
    "    def extract_prediction_probabilities(self, dataset):\n",
    "        if self.pipe is None:\n",
    "            self.find_model_for_dataset(dataset.config_name)\n",
    "        output = self.pipe(dataset['text'])\n",
    "        return np.stack([vectorize(o) for o in output])\n",
    "\n",
    "    def score_dataset(self, dataset):\n",
    "        pred_probs = self.extract_prediction_probabilities(dataset)\n",
    "        cleanlab_suss_score = get_label_quality_scores(\n",
    "            labels=dataset['label'],\n",
    "            pred_probs=pred_probs,  \n",
    "        )\n",
    "        if \"cleanlab_quality_score\" in dataset.features:\n",
    "            dataset = dataset.remove_columns([\"cleanlab_quality_score\"])\n",
    "        dataset = dataset.add_column(\"cleanlab_quality_score\", [s for s in cleanlab_suss_score])\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb311bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tfim_scorer = TFIMQualityScorer(TFIM)\n",
    "uni_dataset = tfim_scorer.score_dataset(uni_dataset)\n",
    "\n",
    "cl_scorer   = CleanLabQualityScorer()\n",
    "uni_dataset = cl_scorer.score_dataset(uni_dataset)\n",
    "\n",
    "uni_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cfee62",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = uni_dataset.to_pandas()\n",
    "df[[\"text\", \"label\", \"tfim_quality_score\", \"cleanlab_quality_score\"]].sort_values(\"tfim_quality_score\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff849bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c_a in np.linspace(0.1, 1, 10):\n",
    "    \n",
    "    c_f = c_g = (1 - c_a) / 2\n",
    "    \n",
    "    print(f\"c_a: {c_a}, c_f: {c_f}, c_g: {c_g}\")\n",
    "    \n",
    "    i = 1\n",
    "    \n",
    "    a = np.load(os.path.join(save_dir, f\"glue.sst2.fada.v3.2.alignment-step-{i}.npy\"))\n",
    "    f = np.load(os.path.join(save_dir, f\"glue.sst2.fada.v3.2.fluency-step-{i}.npy\"))\n",
    "    g = np.load(os.path.join(save_dir, f\"glue.sst2.fada.v3.2.grammar-step-{i}.npy\"))\n",
    "    c = np.load(os.path.join(save_dir, f\"glue.sst2.fada.v3.2.counts-step-{i}.npy\"))\n",
    "    t = np.load(os.path.join(save_dir, f\"glue.sst2.fada.v3.2.tfim-step-{i}.npy\"))\n",
    "    \n",
    "    TFIM = (c_a * a) + (c_f * f) + (c_g * g)\n",
    "    \n",
    "    sns.heatmap(TFIM)\n",
    "    plt.show()\n",
    "    \n",
    "    tfim_scorer = TFIMQualityScorer(TFIM)\n",
    "    uni_dataset = tfim_scorer.score_dataset(uni_dataset)\n",
    "\n",
    "    cl_scorer   = CleanLabQualityScorer()\n",
    "    uni_dataset = cl_scorer.score_dataset(uni_dataset)\n",
    "\n",
    "    df = uni_dataset.to_pandas()\n",
    "    print(f\"correlation with cleanlab: {df['tfim_quality_score'].corr(df['cleanlab_quality_score'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a96ca0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_dataset.to_pandas().to_csv(\"./eval/glue.sst2.uniform.tfim-v3-ranking.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1f79dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18492c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "d, a_scores = a_metric.evaluate_before_and_after(dataset, uni_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
